#!/usr/bin/env python3

# exploreFit.py 				Eric Anderson (3/16)
# A basic fit of generated data, as a sanity check.  We should be able to
# fit pretty well to the generated data, as it is, for the most part, just
# generated by linear interpolation.  We'll separate the data into train,test
# and validation sets using our data splitter helper.

# This file uses several methods for fitting and is intended merely as a proof of concept.


import numpy as np 
import pandas as pd
from sklearn.linear_model import LinearRegression as LR
from sklearn.neighbors import KNeighborsRegressor as KNN
from sklearn.ensemble import RandomForestRegressor as RFR
import sklearn.mixture as Mix

def main():
  np.random.seed(0)       #For repeatability

  #Read in the training data
  print('Reading data from hardcoded file')
  with open('genTrain.csv','r') as f:
    x = pd.read_csv(f)

  #Get a validation set
  print('Creating Validation and Training sets')
  vFrac = 0.3
  print('Splitting off a validation set of size ' + str(vFrac))
  val = x.sample(frac = vFrac)
  x = x[~x.index.isin(val.index)]    #Get the complement

  #Break into x and y
  y = x.loc[:,'boatSpeed']
  yVal = val.loc[:,'boatSpeed']
  x.drop(['boatSpeed'],axis=1,inplace=True); 
  val.drop(['boatSpeed'],axis=1,inplace=True); 

  # print(x.shape)
  # print(val.shape)
  # print(y.shape)
  # print(yVal.shape)
  # print(x.head())
  # print(val.head())
  # print(y.head())
  # print(yVal.head())

  print('--------------------\n      FITS\n--------------------\n')


  ############################
  #   LINEAR REGRESSION      #
  ############################
  '''
  print('Fitting Data with Linear Regression\n...\n')  
  lr = LR().fit(x,y)

  print('Fitting Training Data')
  yhat = lr.predict(x)
  mse = ((yhat-y)**2).mean()
  print('Training Data MSE: ' + str(mse) + '\n')

  print('Fitting Validation Data')
  yhat = lr.predict(val)
  mse = ((yhat-yVal)**2).mean()
  print('Validation Data MSE: ' + str(mse) + '\n')
  '''

  ############################
  #         KNN              #
  ############################
  '''
  n = 20
  print('Fitting Data with KNN Regression (' + str(n) + ' neighbors)\n...\n')
  knn = KNN().fit(x,y)

  print('Fitting Training Data')
  yhat = knn.predict(x)
  mse = ((yhat-y)**2).mean()
  print('Training Data MSE: ' + str(mse) + '\n')

  print('Fitting Validation Data')
  yhat = knn.predict(val)
  mse = ((yhat-yVal)**2).mean()
  print('Validation Data MSE: ' + str(mse) + '\n')
  '''

  ############################
  #      RANDOM FOREST       #
  ############################
  '''
  f = 30
  print('Fitting Data with Random Forest (Forest size of : ' + str(f) + ')\n...\n')
  rf = RFR(n_estimators=f, verbose=2, oob_score=True).fit(x,y)

  print('Fitting Training Data')
  yhat = rf.predict(x)
  mse = ((yhat-y)**2).mean()
  print('Training Data MSE: ' + str(mse) + '\n')

  print('Fitting Validation Data')
  yhat = rf.predict(val)
  mse = ((yhat-yVal)**2).mean()
  print('Validation Data MSE: ' + str(mse) + '\n')
  '''

  ############################
  #        DPGMM             #
  ############################

  n = 10
  print('Fitting Data with DPGMM (' +str(n) + ' components maximum)\n...\n')
  dpgmm = Mix.DPGMM(n_components=n, covariance_type='diag',verbose=0).fit(x,y )  #Can change verbosity but it's VERY verbose

  print('Fitting Training Data')
  yhat = dpgmm.predict(x)
  print(yhat[:10])
  ##FIXME: output from predict is just what gaussian it belongs to...

  print('Fitting Validation Data')
  yhat = dpgmm.predict(val)
  #mse = ((yhat-yVal)**2).mean()
  #print('Validation Data MSE: ' + str(mse) + '\n')


  #TODO: Test data here, eventually?


if __name__ == "__main__":
  main()